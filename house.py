import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.linear_model import Lassofrom sklearn.model_selection import cross_val_scorefrom sklearn.ensemble import RandomForestRegressorfrom xgboost import XGBRegressorfrom sklearn.decomposition import PCA# index_col=0即第0号列作为index值train_data = pd.read_csv("C:/Users/86150/Desktop/data2/train.csv", index_col=0)test_data = pd.read_csv("C:/Users/86150/Desktop/data2/test.csv", index_col=0)print(train_data.shape)# log1p=log(x+1),这行代码用原始数据中的房价和取对数后的房价创建了个新的DataFrame# 在数据预处理时首先可以对偏度比较大的数据用log1p函数进行转化,使其更加服从高斯分布# log1p可以避免出现负数结果# 将结果绘图prices = pd.DataFrame({"price": train_data["SalePrice"], "log(price + 1)": np.log1p(train_data["SalePrice"])})prices.hist()plt.show()plt.close()# 把train_data的特征SalePrice先抽出来,然后将train_data和test_data拼合起来一起处理其他特征y_train = np.log1p(train_data.pop("SalePrice"))all_data = pd.concat((train_data, test_data), axis=0)print(all_data.shape)# MSSubClass值是一个类别,它们之间没有大小关系,但是pandas默认将其处理成数字,将其转换成strall_data["MSSubClass"] = all_data["MSSubClass"].astype(str)# 使用get_dummies方法将MSSubClass转成one-hot形式编码# prefix可以是字符串或字符串列表,这样就把MSSubClass这列扩展成16列的one-hot编码# print(pd.get_dummies(all_df["MSSubClass"], prefix="MSSubClass").head())# 我们可以将all_df中所有表示类别的特征都转成one-hot形式编码all_dummy_data = pd.get_dummies(all_data)# [5 rows x 303 columns]# isnull()判断哪一列有缺失值,.sum()统计缺失值有多少个,ascending=False表示降序排列# 求所有列平均值mean_cols = all_dummy_data.mean()# 缺失值用对应列的均值填充all_dummy_data = all_dummy_data.fillna(mean_cols)# 检查是否有缺失值# print(all_dummy_df.isnull().sum().sum())# 对数据进行归一化,注意one_hot编码形式的数据不需要归一化numeric_cols = all_data.columns[all_data.dtypes != "object"]# 得到需要进行归一化的特征列表# print(numeric_cols)# 求均值和标准差,然后进行标准化numeric_col_means = all_dummy_data.loc[:, numeric_cols].mean()numeric_col_std = all_dummy_data.loc[:, numeric_cols].std()all_dummy_data.loc[:, numeric_cols] = (all_dummy_data.loc[:, numeric_cols] - numeric_col_means) / numeric_col_std#使用PCA方法进行降维pca = PCA(n_components=20)pca.fit(all_dummy_data)# 数据重新分回训练集和测试集dummy_train_data = all_dummy_data.loc[train_data.index]dummy_test_data = all_dummy_data.loc[test_data.index]# .values将dataframe对象转换成numpy array形式X_train = dummy_train_data.valuesX_test = dummy_test_data.valuesfrom sklearn.linear_model import LassoCVlassocv = LassoCV()lassocv.fit(X_train, y_train)alpha = lassocv.alpha_print('利用Lasso交叉检验计算得出的最优alpha：' + str(alpha))# 设定随机森林中的决策树使用的特征占比max_features = [.1, .3, .5, .7, .9, ]test_scores = []# 网格搜索来寻找最佳max_feat# 使用随机森林模型预测for max_feat in max_features:   # n_estimators为最大弱学习器的个数(决策树的个数),max_features=max_feat即决策树使用的特征占所有特征的比例   clf = RandomForestRegressor(n_estimators=100, max_features=max_feat)   # 交叉验证   test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=5, scoring="neg_mean_squared_error"))   # 记录每个alpha值使用交叉验证时得到的test_score平均值   test_scores.append(np.mean(test_score))plt.plot(max_features, test_scores)plt.xlabel("max_features")plt.ylabel("test_scores")plt.show()plt.close()params = [1, 2, 3, 4, 5, 6]test_scores = []for param in params:   # xgboost是梯度提升树的实现,XGBRegressor默认使用CART回归树,max_depth是回归树的最大深度,我们这里通过网格搜索找出最佳max_depth   clf = XGBRegressor(max_depth=param)   test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring="neg_mean_squared_error"))   test_scores.append(np.mean(test_score))plt.plot(params, test_scores)plt.xlabel("params")plt.ylabel("test scores")plt.show()plt.close()# 根据上面的搜索,使用拉索回归时最佳alpha为0.0006,使用随机森林时最佳max_features=0.3 XGBRegressor为2lasso = Lasso(0.0006)rf = RandomForestRegressor(n_estimators=100, max_features=.3)xgboost_model = XGBRegressor(max_depth=2)print('训练完成')# 模型学习lasso.fit(X_train, y_train)rf.fit(X_train, y_train)xgboost_model.fit(X_train, y_train)# 模型预测y_lasso = np.expm1(lasso.predict(X_test))y_rf = np.expm1(rf.predict(X_test))y_xgb = np.expm1(xgboost_model.predict(X_test))# baggingy_final = (y_lasso + y_rf + y_xgb) / 3# submission_data即最后的预测结果submission_data = pd.DataFrame(data={"Id": test_data.index, "SalePrice": y_final})print('预测完成')submission_data.to_csv("C:/Users/86150/Desktop/data2/submission.csv", index=False)